{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Operando* pipeline P02.1, PETRA III, DESY, August 2023\n",
    "\n",
    "**NB**: this pipeline is customized for scattering data from the Ravnsb√¶k Group\n",
    "beamtime at beamline P02.1, PETRA III, DESY, in August 2021.  \n",
    "The customization is solely due to the data filenames containing a timestamp,\n",
    "which made them incompatible with the parent pipeline.\n",
    "\n",
    "Please go though this notebook cell by cell to obtain azimuthally integrated\n",
    "data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider inspecting your data files (e.g., `.tif` files) using the `silx view` \n",
    "gui below.  \n",
    "Doing so, you'll be able to identify 'outliers' from, e.g., beamdumps, etc.  \n",
    "If not relevant, just proceed without running the cell below or just close the\n",
    "gui soon as it opens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! silx view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case you found any 'outliers', please put them in a separate folder.  \n",
    "\n",
    "Then you'll know when and where to include blank scans for your overview plots \n",
    "later in time.  \n",
    "This will allow you to line up electrochemistry with the scattering data later \n",
    "on."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.io\n",
    "from skbeam.core.utils import q_to_twotheta\n",
    "import fabio\n",
    "import pyFAI\n",
    "from pyFAI.azimuthalIntegrator import AzimuthalIntegrator"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please state the format of the files to be integrated, e.g., `tif` or `npy`.  \n",
    "Also, please state the output format, e.g., `chi`, `dat`, `xy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_format = \"tif\"\n",
    "output_format = \"chi\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating directories for input files:\n",
    "- `*input_format*` for input files to be integrated. Stated above.\n",
    "- `*input_format*_bkg` for background file. \n",
    "- `*input_format*_calib` for crystalline standard for calibration file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = Path.cwd() / input_format\n",
    "input_bkg_path = Path.cwd() / f\"{input_format}_bkg\"\n",
    "input_calib_path = Path.cwd() / f\"{input_format}_calib\"\n",
    "input_paths = [input_path, input_bkg_path, input_calib_path]\n",
    "for p in input_paths:\n",
    "    if not p.exists():\n",
    "        p.mkdir()\n",
    "        print(f\"{80*'-'}\\nA folder called '{p.name}' has been created.\\n\"\n",
    "              f\"Please put the appropriate .{input_format} file(s) in the \"\n",
    "              f\"folder before proceeding.\"\n",
    "              )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking whether input files are present in the input folders and collecting\n",
    "them if they are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_all_not_found = False\n",
    "for p in input_paths:\n",
    "    if len(list(p.glob(f\"*.{input_format}\"))) == 0:\n",
    "        inputs_all_not_found = True\n",
    "        print(f\"{80*'-'}\\nNo .{input_format} files were found in the \"\n",
    "              f\"'{p.name}' folder.\\nPlease place the appropriate \"\n",
    "              f\".{input_format} file(s) and rerun before proceeding.\"\n",
    "              )\n",
    "if inputs_all_not_found:\n",
    "    sys.exit()\n",
    "else:\n",
    "    input_files = list(input_path.glob(f\"*.{input_format}\"))\n",
    "    input_bkg_file = list(input_bkg_path.glob(f\"*.{input_format}\"))[0]\n",
    "    input_calib_file = list(input_calib_path.glob(f\"*.{input_format}\"))[0]\n",
    "    print(f\"{80*'-'}\\nInput files collected.\\n{80*'-'}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating directories for output files from calibration in pyFAI (below.)\n",
    "- `npy_mask` for .npy file for static mask during calibration.\n",
    "- `npt_calib` for .npt file containing azimuthal rings used for calibration.\n",
    "- `poni_calib` for the .ponu file containing the experimental geometry obtained\n",
    "  from the calibration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npy_mask_path = Path.cwd() / \"npy_mask\"\n",
    "npt_calib_path = Path.cwd() / \"npt_calib\"\n",
    "poni_calib_path = Path.cwd() / \"poni_calib\"\n",
    "calib_paths = [npy_mask_path, npt_calib_path, poni_calib_path]\n",
    "for p in calib_paths:\n",
    "    if not p.exists():\n",
    "        p.mkdir()\n",
    "        print(f\"{80*'-'}\\nA folder called '{p.name}' has been created.\\n\"\n",
    "              f\"Please save a .{p.name.split('_')[0]} file in the folder \"\n",
    "              f\"during the calibration in pyFAI (below).\"\n",
    "              )        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pyfai-calib2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collecting calibration files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calibs_all_not_found = False\n",
    "calib_paths = [npy_mask_path, poni_calib_path]\n",
    "for p in calib_paths:\n",
    "    if len(list(p.glob(f\"*.{p.name.split('_')[0]}\"))) == 0:\n",
    "        calibs_all_not_found = True\n",
    "        print(f\"{80*'-'}\\nNo .{p.name.split('_')[0]} file was found in the \"\n",
    "              f\"'{p.name}' folder.\\nPlease place the appropriate \"\n",
    "              f\".{p.name.split('_')[0]} file and rerun before proceeding.\"\n",
    "              )\n",
    "if calibs_all_not_found:\n",
    "    print(f\"{80*'-'}\")\n",
    "    sys.exit()\n",
    "else:\n",
    "    npy_mask_file = list(npy_mask_path.glob(f\"*.npy\"))[0]\n",
    "    poni_file = list(poni_calib_path.glob(f\"*.poni\"))[0]\n",
    "    print(f\"{80*'-'}\\nCalibration files collected.\\n{80*'-'}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting static mask from .npy format to .msk fotmat. (Fit2D mask, which is \n",
    "also compatible with the DAWN software package.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msk_static_path = Path.cwd() / \"msk_static\"\n",
    "if not msk_static_path.exists():\n",
    "    msk_static_path.mkdir()\n",
    "msk_static_output = msk_static_path / f\"{npy_mask_file.stem}.msk\"\n",
    "fabio.open(npy_mask_file).convert(\"msk\").save(msk_static_output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating output drectories.\n",
    "- `*output_format*` for azimuthally integrated files. Stated above.\n",
    "- `*output_format*_bkg` for azimuthally integrated background file.\n",
    "- `*output_format*_calib` for azimuthally integrated calibration file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = Path.cwd() / output_format\n",
    "output_bkg_path = Path.cwd() / f\"{output_format}_bkg\"\n",
    "output_calib_path = Path.cwd() / f\"{output_format}_calib\"\n",
    "output_paths = [output_path, output_bkg_path, output_calib_path]\n",
    "for p in output_paths:\n",
    "    if not p.exists():\n",
    "        p.mkdir()\n",
    "        print(f\"{80*'-'}\\nA folder called '{p.name}' has been created for \"\n",
    "              f\"azimuthally integrated .{output_format} file(s).\"\n",
    "              )    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports and function definitions for automasking and integration.\n",
    "\n",
    "Adopted from: https://github.com/st3107/masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from numpy import ndarray\n",
    "from numba import jit, boolean\n",
    "from skbeam.core.accumulators.binned_statistic import BinnedStatistic1D\n",
    "from skbeam.core.mask import margin\n",
    "\n",
    "@jit(cache=True, nopython=True, nogil=True)\n",
    "def mask_ring_median(values_array, positions_array, alpha):  # pragma: no cover\n",
    "    \"\"\"Find outlier pixels in a single ring via a single pass with the median.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    values_array : ndarray\n",
    "        The ring values\n",
    "    positions_array : ndarray\n",
    "        The positions of the values\n",
    "    alpha: float\n",
    "        The threshold\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    removals: np.ndarray\n",
    "        The positions of pixels to be removed from the data\n",
    "    \"\"\"\n",
    "    z = np.abs(values_array - np.median(values_array)) / np.std(values_array)\n",
    "    removals = positions_array[np.where(z > alpha)]\n",
    "    return removals\n",
    "\n",
    "\n",
    "@jit(cache=True, nopython=True, nogil=True)\n",
    "def mask_ring_mean(values_array, positions_array, alpha):  # pragma: no cover\n",
    "    \"\"\"Find outlier pixels in a single ring via a pixel by pixel method with\n",
    "    the mean.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    values_array : ndarray\n",
    "        The ring values\n",
    "    positions_array : ndarray\n",
    "        The positions of the values\n",
    "    alpha: float\n",
    "        The threshold\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    removals: np.ndarray\n",
    "        The positions of pixels to be removed from the data\n",
    "    \"\"\"\n",
    "    m = np.ones(positions_array.shape, dtype=boolean)\n",
    "    removals = []\n",
    "    while True:\n",
    "        b = np.array(\n",
    "            [item in removals for item in positions_array], dtype=boolean\n",
    "        )\n",
    "        m[b] = False\n",
    "        v = values_array[m]\n",
    "        if len(v) <= 1:\n",
    "            break\n",
    "        std = np.std(v)\n",
    "        if std == 0.0:\n",
    "            break\n",
    "        norm_v_list = np.abs(v - np.mean(v)) / std\n",
    "        if np.all(norm_v_list < alpha):\n",
    "            break\n",
    "        # get the index of the worst pixel\n",
    "        worst_idx = np.argmax(norm_v_list)\n",
    "        # add the worst position to the mask\n",
    "        removals.append(positions_array[m][worst_idx])\n",
    "    return removals\n",
    "\n",
    "\n",
    "mask_ring_dict = {\"median\": mask_ring_median, \"mean\": mask_ring_mean}\n",
    "\n",
    "def map_to_binner(pixel_map, bins, mask=None):\n",
    "    \"\"\"Transforms pixel map and bins into a binner\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    pixel_map: np.ndarray\n",
    "        The map between pixels and values\n",
    "    bins: np.ndarray\n",
    "        The bins to use in the binner\n",
    "    mask: np.ndarray, optional\n",
    "        The mask for the pixel map\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    BinnedStatistic1D:\n",
    "        The binner\n",
    "\n",
    "    \"\"\"\n",
    "    if mask is not None:\n",
    "        mask = mask.flatten()\n",
    "\n",
    "    return BinnedStatistic1D(pixel_map.flatten(), bins=bins, mask=mask)\n",
    "\n",
    "\n",
    "def generate_binner(geo, img_shape, mask=None):\n",
    "    \"\"\"Create a pixel resolution BinnedStats1D instance\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    geo : pyFAI.geometry.Geometry instance\n",
    "        The calibrated geometry\n",
    "    img_shape : tuple, optional\n",
    "        The shape of the image, if None pull from the mask. Defaults to None.\n",
    "    mask : np.ndarray, optional\n",
    "        The mask to be applied, if None no mask is applied. Defaults to None.\n",
    "    Returns\n",
    "    -------\n",
    "    BinnedStatistic1D :\n",
    "        The configured instance of the binner.\n",
    "    \"\"\"\n",
    "    return map_to_binner(*generate_map_bin(geo, img_shape), mask=mask)\n",
    "\n",
    "\n",
    "def binned_outlier(\n",
    "    img, binner, tmsk, alpha=3, mask_method=\"median\", pool=None\n",
    "):\n",
    "    \"\"\"Sigma Clipping based masking.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    img : np.ndarray\n",
    "        The image\n",
    "    binner : BinnedStatistic1D instance\n",
    "        The binned statistics information\n",
    "    alpha : float, optional\n",
    "        The number of standard deviations to clip, defaults to 3\n",
    "    tmsk : np.ndarray, optional\n",
    "        Prior mask. If None don't use a prior mask, defaults to None.\n",
    "    mask_method : {'median', 'mean'}, optional\n",
    "        The method to use for creating the mask, median is faster, mean is more\n",
    "        accurate. Defaults to median.\n",
    "    pool : Executor instance\n",
    "        A pool against which jobs can be submitted for parallel processing\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray:\n",
    "        The mask\n",
    "    \"\"\"\n",
    "    if pool is None:\n",
    "        pool = ThreadPoolExecutor(max_workers=20)\n",
    "    # skbeam 0.0.12 doesn't have argsort_index cached\n",
    "    idx = binner.argsort_index\n",
    "    tmsk = tmsk.flatten()\n",
    "    tmsk2 = tmsk[idx]\n",
    "    vfs = img.flatten()[idx]\n",
    "    pfs = np.arange(np.size(img))[idx]\n",
    "    t = []\n",
    "    i = 0\n",
    "    for k in binner.flatcount:\n",
    "        m = tmsk2[i: i + k]\n",
    "        vm = vfs[i: i + k][m]\n",
    "        if k > 0 and len(vm) > 0:\n",
    "            t.append((vm, (pfs[i: i + k][m]), alpha))\n",
    "        i += k\n",
    "    p_err = np.seterr(all=\"ignore\")\n",
    "    # only run tqdm on mean since it is slow\n",
    "    with pool as p:\n",
    "        futures = [\n",
    "            p.submit(mask_ring_dict[mask_method], *x)\n",
    "            for x in t\n",
    "        ]\n",
    "    removals = []\n",
    "    for f in as_completed(futures):\n",
    "        removals.extend(f.result())\n",
    "    np.seterr(**p_err)\n",
    "    tmsk[removals] = False\n",
    "    tmsk = tmsk.reshape(np.shape(img))\n",
    "    \n",
    "    return tmsk.astype(bool)\n",
    "\n",
    "\n",
    "def generate_map_bin(geo, img_shape):\n",
    "    \"\"\"Create a q map and the pixel resolution bins\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    geo : pyFAI.geometry.Geometry instance\n",
    "        The calibrated geometry\n",
    "    img_shape : tuple, optional\n",
    "        The shape of the image, if None pull from the mask. Defaults to None.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    q : ndarray\n",
    "        The q map\n",
    "    qbin : ndarray\n",
    "        The pixel resolution bins\n",
    "    \"\"\"\n",
    "    r = geo.rArray(img_shape)\n",
    "    q = geo.qArray(img_shape) / 10  # type: np.ndarray\n",
    "    q_dq = geo.deltaQ(img_shape) / 10  # type: np.ndarray\n",
    "\n",
    "    pixel_size = [getattr(geo, a) for a in [\"pixel1\", \"pixel2\"]]\n",
    "    rres = np.hypot(*pixel_size)\n",
    "    rbins = np.arange(np.min(r) - rres / 2., np.max(r) + rres / 2., rres / 2.)\n",
    "    rbinned = BinnedStatistic1D(r.ravel(), statistic=np.max, bins=rbins)\n",
    "\n",
    "    qbin_sizes = rbinned(q_dq.ravel())\n",
    "    qbin_sizes = np.nan_to_num(qbin_sizes)\n",
    "    qbin = np.cumsum(qbin_sizes)\n",
    "    qbin[0] = np.min(q_dq)\n",
    "    if np.max(q) > qbin[-1]:\n",
    "        qbin[-1] = np.max(q)\n",
    "    \n",
    "    return q, qbin\n",
    "\n",
    "\n",
    "def mask_img(\n",
    "    img,\n",
    "    binner,\n",
    "    edge=30,\n",
    "    lower_thresh=0.0,\n",
    "    upper_thresh=None,\n",
    "    alpha=2,\n",
    "    auto_type=\"median\",\n",
    "    tmsk=None,\n",
    "    pool=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Mask an image based off of various methods\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    img: np.ndarray\n",
    "        The image to be masked\n",
    "    binner : BinnedStatistic1D instance\n",
    "        The binned statistics information\n",
    "    edge: int, optional\n",
    "        The number of edge pixels to mask. Defaults to 30. If None, no edge\n",
    "        mask is applied\n",
    "    lower_thresh: float, optional\n",
    "        Pixels with values less than or equal to this threshold will be masked.\n",
    "        Defaults to 0.0. If None, no lower threshold mask is applied\n",
    "    upper_thresh: float, optional\n",
    "        Pixels with values greater than or equal to this threshold will be\n",
    "        masked.\n",
    "        Defaults to None. If None, no upper threshold mask is applied.\n",
    "    alpha: float, optional\n",
    "        The number of acceptable standard deviations, if tuple then we use\n",
    "        a linear distribution of alphas from alpha[0] to alpha[1], if array\n",
    "        then we just use that as the distribution of alphas. Defaults to 3.\n",
    "        If None, no outlier masking applied.\n",
    "    auto_type: {'median', 'mean'}, optional\n",
    "        The type of binned outlier masking to be done, 'median' is faster,\n",
    "        where 'mean' is more accurate, defaults to 'median'.\n",
    "    tmsk: np.ndarray, optional\n",
    "        The starting mask to be compounded on. Defaults to None. If None mask\n",
    "        generated from scratch.\n",
    "    pool : Executor instance\n",
    "        A pool against which jobs can be submitted for parallel processing\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tmsk: np.ndarray\n",
    "        The mask as a boolean array. True pixels are good pixels, False pixels\n",
    "        are masked out.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if tmsk is None:\n",
    "        working_mask = np.ones(np.shape(img)).astype(bool)\n",
    "    else:\n",
    "        working_mask = tmsk.copy()\n",
    "    if edge:\n",
    "        working_mask *= margin(np.shape(img), edge)\n",
    "    if lower_thresh is not None:\n",
    "        working_mask *= (img >= lower_thresh).astype(bool)\n",
    "    if upper_thresh is not None:\n",
    "        working_mask *= (img <= upper_thresh).astype(bool)\n",
    "    if alpha:\n",
    "        working_mask *= binned_outlier(\n",
    "            img,\n",
    "            binner,\n",
    "            alpha=alpha,\n",
    "            tmsk=working_mask,\n",
    "            mask_method=auto_type,\n",
    "            pool=pool,\n",
    "        )\n",
    "    working_mask = working_mask.astype(bool)\n",
    "    \n",
    "    return working_mask\n",
    "\n",
    "\n",
    "def integrate(\n",
    "    img: ndarray, ai: AzimuthalIntegrator, mask: ndarray = None, integ_setting: dict = None\n",
    ") -> Tuple[ndarray, dict]:\n",
    "    \"\"\"Use AzimuthalIntegrator to integrate the image.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    img : ndarray\n",
    "        The 2D diffraction image array.\n",
    "\n",
    "    ai : AzimuthalIntegrator\n",
    "        The AzimuthalIntegrator instance.\n",
    "\n",
    "    mask : ndarray\n",
    "        The mask as a 0 and 1 array. 0 pixels are good pixels, 1 pixels are masked out.\n",
    "\n",
    "    integ_setting : dict\n",
    "        The user's modification to integration settings.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    chi : ndarray\n",
    "        The chi data. The first row is bin centers and the second row is the average intensity in bins.\n",
    "\n",
    "    _integ_setting: dict\n",
    "        The whole integration setting.\n",
    "    \"\"\"\n",
    "    # merge integrate setting\n",
    "    _integ_setting = INTEG_SETTING.copy()\n",
    "    if integ_setting is not None:\n",
    "        _integ_setting.update(integ_setting)\n",
    "    # integrate\n",
    "    xy = ai.integrate1d(img, mask=mask, **_integ_setting)\n",
    "    chi = np.stack(xy)\n",
    "\n",
    "    return chi, _integ_setting\n",
    "\n",
    "\n",
    "def auto_mask(\n",
    "    img: ndarray,\n",
    "    ai: AzimuthalIntegrator,\n",
    "    user_mask: ndarray = None,\n",
    "    mask_setting: dict = None\n",
    ") -> ndarray:\n",
    "    \"\"\"Automatically generate the mask of the image.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    img : ndarray\n",
    "        The 2D diffraction image array.\n",
    "\n",
    "    ai : AzimuthalIntegrator\n",
    "        The AzimuthalIntegrator instance.\n",
    "\n",
    "    mask_setting : dict\n",
    "        The user's modification to auto-masking settings.\n",
    "\n",
    "    user_mask : ndarray\n",
    "        A mask provided by user. It is an integer array. 0 are good pixels, 1 are masked out.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    mask : ndarray\n",
    "        The mask as an integer array. 0 are good pixels, 1 are masked out.\n",
    "    \"\"\"\n",
    "    if mask_setting is not None:\n",
    "        _mask_setting = mask_setting\n",
    "    else:\n",
    "        _mask_setting = dict()\n",
    "    binner = generate_binner(ai, img.shape)\n",
    "    tmsk = user_mask.astype(bool) if user_mask is not None else None\n",
    "    mask = mask_img(img, binner, tmsk=tmsk, **_mask_setting)\n",
    "    mask = mask.astype(int)\n",
    "    \n",
    "    return mask"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Settings for the azimuthal integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INTEG_SETTING = dict(npt=2000,\n",
    "                     polarization_factor=0.9,\n",
    "                     correctSolidAngle=True,\n",
    "                     method='splitpixel',\n",
    "                     unit='q_A^-1',\n",
    "                     safe=True,\n",
    "                    #  radial_range=(1.0, 7.3),\n",
    "                     )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Azimuthal integration of calibrant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "if input_format == \"npy\":\n",
    "    data_calib = np.load(input_calib_file)\n",
    "else:\n",
    "      data_calib = skimage.io.imread(input_calib_file)\n",
    "ai = pyFAI.load(str(poni_file))\n",
    "xy_calib, _integ_setting = integrate(data_calib,\n",
    "                                     ai=pyFAI.load(str(poni_file)),\n",
    "                                     mask=np.load(npy_mask_file),\n",
    "                                     integ_setting=INTEG_SETTING,\n",
    "                                     )\n",
    "np.savetxt(output_calib_path / f\"{input_calib_file.stem}.{output_format}\",\n",
    "           xy_calib.T,\n",
    "           header=\"q[AA^-1]\\tI[arb. u]\", \n",
    "           delimiter=\"\\t\",\n",
    "           encoding=\"utf-8\",\n",
    "           )\n",
    "print(f\"{80*'-'}\\nAzimuthally integrated calibrant data saved to the \"\n",
    "      f\"'{output_calib_path.name}' folder.\\n{80*'-'}\")\n",
    "x, y = xy_calib\n",
    "d_calib = dict(x=x, y=y)\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "ax.plot(x, y)\n",
    "ax.set_xlim(np.amin(x), np.amax(x))\n",
    "ax.set_xlabel(\"$Q\\;[\\mathrm{\\AA}^{-1}]$\")\n",
    "ax.set_ylabel(\"$I\\;[\\mathrm{arb.\\;u.}]$\")\n",
    "ax.set_title(input_calib_file.name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Azimuthal integration of background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "if input_format == \"npy\":\n",
    "    data_bkg = np.load(input_bkg_file)\n",
    "else:\n",
    "     data_bkg = skimage.io.imread(input_bkg_file)\n",
    "ai = pyFAI.load(str(poni_file))\n",
    "xy_bkg, _integ_setting = integrate(data_bkg,\n",
    "                                   ai=pyFAI.load(str(poni_file)),\n",
    "                                   mask=np.load(npy_mask_file),\n",
    "                                   integ_setting=INTEG_SETTING,\n",
    "                                   )\n",
    "np.savetxt(output_bkg_path / f\"{input_bkg_file.stem}.{output_format}\",\n",
    "           xy_bkg.T,\n",
    "           header=\"q[AA^-1]\\tI[arb. u]\", \n",
    "           delimiter=\"\\t\",\n",
    "           encoding=\"utf-8\",\n",
    "           )\n",
    "print(f\"{80*'-'}\\nAzimuthally integrated data saved to the \"\n",
    "      f\"'{output_bkg_path.name}' folder.\\n{80*'-'}\")\n",
    "x, y = xy_bkg\n",
    "d_bkg = dict(x=x, y=y)\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "ax.plot(x, y)\n",
    "ax.set_xlim(np.amin(x), np.amax(x))\n",
    "ax.set_xlabel(\"$Q\\;[\\mathrm{\\AA}^{-1}]$\")\n",
    "ax.set_ylabel(\"$I\\;[\\mathrm{arb.\\;u.}]$\")\n",
    "ax.set_title(input_bkg_file.name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Settings used for the automasking.\n",
    "\n",
    "The key parameter to tune is the `alpha` parameter. (See definition below.)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "edge: int, optional\n",
    "    The number of edge pixels to mask. Defaults to 30. If None, no edge\n",
    "    mask is applied\n",
    "lower_thresh: float, optional\n",
    "    Pixels with values less than or equal to this threshold will be masked.\n",
    "    Defaults to 0.0. If None, no lower threshold mask is applied\n",
    "upper_thresh: float, optional\n",
    "    Pixels with values greater than or equal to this threshold will be\n",
    "    masked.\n",
    "    Defaults to None. If None, no upper threshold mask is applied.\n",
    "alpha: float, optional\n",
    "    The number of acceptable standard deviations, if tuple then we use\n",
    "    a linear distribution of alphas from alpha[0] to alpha[1], if array\n",
    "    then we just use that as the distribution of alphas. Defaults to 3.\n",
    "    If None, no outlier masking applied.\n",
    "auto_type: {'median', 'mean'}, optional\n",
    "    The type of binned outlier masking to be done, 'median' is faster,\n",
    "    where 'mean' is more accurate, defaults to 'median'.\n",
    "\"\"\"\n",
    "MASK_SETTING = dict(edge=10, \n",
    "                    lower_thresh=0, \n",
    "                    upper_thresh=None, \n",
    "                    alpha=5,\n",
    "                    auto_type=\"mean\",\n",
    "                    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Automasking the first .tif file and plotting to see whether automasking settings\n",
    "are appropriate.  \n",
    "If not, please edit the mask settings above until appropariate automasking is \n",
    "observed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "input_file = input_files[0]\n",
    "print(f\"{80*'-'}\\nAutomasking: {input_file.name}\\n\\tmask settings:\")\n",
    "for k, v in MASK_SETTING.items():\n",
    "    print(f\"\\t\\t{k}\\t{v}\")\n",
    "mask_static = np.load(npy_mask_file)\n",
    "if input_format == \"npy\":\n",
    "    data = np.load(input_file)\n",
    "else:\n",
    "    data = skimage.io.imread(input_file)\n",
    "ai = pyFAI.load(str(poni_file))\n",
    "mask_dynamic = auto_mask(data,\n",
    "                         ai,\n",
    "                         1 - mask_static,\n",
    "                         mask_setting=MASK_SETTING,\n",
    "                        ) \n",
    "data_masked = data * (1 - mask_static) * mask_dynamic\n",
    "data_masked[data_masked < 1] = -1\n",
    "data_masked = np.ma.masked_where(data_masked == -1, data_masked)\n",
    "cmap = mpl.colormaps[\"binary\"]\n",
    "cmap.set_bad(color=\"r\")\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "im = ax.imshow(data_masked, \n",
    "               aspect=\"equal\", \n",
    "               cmap=cmap,\n",
    "               )\n",
    "ax.set_xlabel(\"$x\\;[\\mathrm{pixels}]$\")\n",
    "ax.set_ylabel(\"$y\\;[\\mathrm{pixels}]$\")\n",
    "ax.set_title(input_file.name)\n",
    "cbar = fig.colorbar(im)\n",
    "cbar.set_label(\"$I\\;[\\mathrm{arb.\\;u.}]$\")\n",
    "print(f\"Done automasking.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sorting all data files according to running number.  \n",
    "**NB**: This is a specific step for this customized pipeline, compared to the \n",
    "parent pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_files = {}\n",
    "for f in input_files:\n",
    "    no = int(f.stem.split(\"-\")[-1])\n",
    "    d_files[no] = f\n",
    "keys_sorted = sorted(list(d_files.keys()))\n",
    "input_files = [d_files[k] for k in keys_sorted]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Automasking all .tif files.  \n",
    "**NB**: this is a time-consuming step, as the automasking routine includes a \n",
    "preliminary azimuthal integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{80*'-'}\\nAutomasking .{input_format} files...\")\n",
    "d_data = {}\n",
    "mask_static = np.load(npy_mask_file)\n",
    "for i, input_file in enumerate(input_files):\n",
    "    print(f\"\\t{i}\\t{input_file.name}\")\n",
    "    if input_format == \"npy\":\n",
    "        data = np.load(input_file)\n",
    "    else:\n",
    "        data = skimage.io.imread(input_file)\n",
    "    mask_dynamic = auto_mask(img=data,\n",
    "                             ai=pyFAI.load(str(poni_file)),\n",
    "                             user_mask=1-mask_static,\n",
    "                             mask_setting=MASK_SETTING,\n",
    "                            )\n",
    "    d_data[i] = dict(input_file=input_file, mask_dynamic=mask_dynamic)\n",
    "print(f\"Done automasking .{input_format} files.\\n{80*'-'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting new output file names by discarding the time part of the parent file \n",
    "name.  \n",
    "**NB**: this is a specific step for this customized pipeline, compared to the \n",
    "parent pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_keys = list(d_data.keys())\n",
    "zfill = len(str(len(data_keys)))\n",
    "for i, k in enumerate(data_keys):\n",
    "    input_file = d_data[k][\"input_file\"]\n",
    "    no = input_file.stem.split(\"-\")[-1]\n",
    "    output_name = \"_\".join(input_file.stem.split(\"_\")[0:4])\n",
    "    output_name = f\"{output_name}_{str(i).zfill(zfill)}-{no}\"\n",
    "    d_data[k][\"output_name\"] = output_name"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to save the dynamic masks from above to `.npy` and `.msk` files,\n",
    "please run the cell below.  \n",
    "`Otherwise just skip it, as it might take up quite some memory.`\n",
    "\n",
    "The .msk format is used by the Fit2D software but can also be used by the DAWN\n",
    "software package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npy_mask_dynamic_path = Path.cwd() / \"npy_mask_dynamic\"\n",
    "msk_dynamic_path = Path.cwd() / \"msk_dynamic\"\n",
    "for p in [npy_mask_dynamic_path, msk_dynamic_path]:\n",
    "    if not p.exists():\n",
    "        p.mkdir()\n",
    "print(f\"{80*'-'}\\nSaving dynamic masks as .npy and .msk files...\")\n",
    "data_keys = list(d_data.keys())\n",
    "zfill = len(str(len(data_keys)))\n",
    "for i, k in enumerate(data_keys):\n",
    "    input_file = d_data[k][\"input_file\"]\n",
    "    output_name = d_data[k][\"output_name\"]\n",
    "    mask_dynamic = d_data[k][\"mask_dynamic\"]\n",
    "    print(f\"\\t{i}\\t{input_file.name}\")\n",
    "    np.save(npy_mask_dynamic_path / f\"{output_name}_mask\",\n",
    "            1 - mask_dynamic,\n",
    "            )\n",
    "    npy_mask_path = npy_mask_dynamic_path / f\"{output_name}_mask.npy\"\n",
    "    msk_dynamic_output = msk_dynamic_path / f\"{output_name}_mask.msk\"\n",
    "    fabio.open(npy_mask_path).convert(\"msk\").save(msk_dynamic_output)\n",
    "print(f\"Done. Please see the '{npy_mask_dynamic_path.name}' and \"\n",
    "      f\"'{msk_dynamic_path.name}' folders.\\n{80*'-'}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Integration and plotting of the first input data file to see, whether the \n",
    "integration settings are appropriate.  \n",
    "If not, please edit the integration settings above, until appropriate \n",
    "integration is obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "keys = list(d_data.keys())\n",
    "input_file = d_data[keys[0]][\"input_file\"]\n",
    "mask = d_data[keys[0]][\"mask_dynamic\"]\n",
    "print(f\"{80*'-'}\\nIntegrating {input_file.name}\")\n",
    "if input_format == \"npy\":\n",
    "    data = np.load(input_file)\n",
    "else:\n",
    "    data = skimage.io.imread(input_file)\n",
    "xy_data, _integ_setting = integrate(data,\n",
    "                                    ai=pyFAI.load(str(poni_file)),\n",
    "                                    mask=1-mask,\n",
    "                                    integ_setting=INTEG_SETTING,\n",
    "                                    )\n",
    "x, y = xy_data\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "ax.plot(x, y)\n",
    "ax.set_xlim(np.amin(x), np.amax(x))\n",
    "ax.set_xlabel(\"$Q\\;[\\mathrm{\\AA}^{-1}]$\", fontsize=14)\n",
    "ax.set_ylabel(\"$I\\;[\\mathrm{arb.\\;u.}]$\", fontsize=14)\n",
    "ax.set_title(input_file.name, fontsize=14)\n",
    "print(f\"Done integrating.\\n{80*'-'}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Integrating all input files.  \n",
    "**NB**: this is a time-consuming step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{80*'-'}\\nAzimuthally integrating .{input_format} files to \"\n",
    "      f\".{output_format} files...\")\n",
    "for i, k in enumerate(d_data.keys()):\n",
    "    input_file = d_data[k][\"input_file\"]\n",
    "    mask = d_data[k][\"mask_dynamic\"]\n",
    "    output_name = d_data[k][\"output_name\"]\n",
    "    print(f\"\\t{i}\\t{output_name}.{output_format}\")\n",
    "    if input_format == \"npy\":\n",
    "        data = np.load(input_file)\n",
    "    else:\n",
    "        data = skimage.io.imread(input_file)\n",
    "    xy_data, _integ_setting = integrate(data,\n",
    "                                        ai=pyFAI.load(str(poni_file)),\n",
    "                                        mask=1-mask,\n",
    "                                        integ_setting=INTEG_SETTING,\n",
    "                                        )\n",
    "    d_data[k][\"x\"], d_data[k][\"y\"] = xy_data\n",
    "    np.savetxt(output_path / f\"{output_name}.{output_format}\", \n",
    "               xy_data.T,\n",
    "               header=\"q[AA^-1]\\tI[arb. u]\", \n",
    "               delimiter=\"\\t\",\n",
    "               encoding=\"utf-8\",\n",
    "               )\n",
    "print(f\"Done integrating .{input_format} files. Please see the \"\n",
    "      f\"{output_path.name} folder.\\n{80*'-'}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting azimuthally integrated background and data together.\n",
    "\n",
    "`Please use the plot to inspect, which Q-range ('xmin' and 'xmax' below) to base\n",
    "the normalization/scaling on to account for beam intensity fluctuations.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "keys = list(d_data.keys())\n",
    "cmap = mpl.colormaps[\"hsv\"]\n",
    "cmap_idx = np.linspace(0, 1, len(keys))\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "ax.plot(d_bkg[\"x\"], d_bkg[\"y\"], c=\"k\", lw=1, label=\"bkg\")\n",
    "for i, k in enumerate(keys):\n",
    "    if i in range(len(keys)-1):\n",
    "        ax.plot(d_data[k][\"x\"], d_data[k][\"y\"], c=np.random.rand(3,), lw=1)\n",
    "ax.set_xlim(np.amin(d_bkg[\"x\"]), np.amax(d_bkg[\"x\"]))\n",
    "ax.set_xlabel(\"$Q\\;[\\mathrm{\\AA}^{-1}]$\")\n",
    "ax.set_ylabel(\"$I\\;[\\mathrm{arb.\\;u.}]$\")\n",
    "ax.set_title(\"Non-normalized background and data\")\n",
    "ax.legend()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minimum and maximum $Q$-value to base the normalization/scaling on to account\n",
    "for beam intensity fluctuations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmin, xmax = 1.0, 1.05"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating folders for output files with estimated standard deviations (esd)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_esd_path = Path.cwd() / f\"{output_format}_esd\"\n",
    "bkg_esd_path = Path.cwd() / f\"{output_format}_bkg_esd\"\n",
    "calib_esd_path = Path.cwd() / f\"{output_format}_calib_esd\"\n",
    "for p in [data_esd_path, bkg_esd_path, calib_esd_path]:\n",
    "    if not p.exists():\n",
    "        p.mkdir()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to estimate standard deviations and writing to files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def esd_calc_write(filename, x, y, wl, sdd, output_path):\n",
    "    x_rad = q_to_twotheta(x, wl)\n",
    "    normalizer = np.array([2*np.pi*sdd*np.tan(x_rad[i])/0.150\n",
    "                           for i in range(len(x_rad))])\n",
    "    esd = np.sqrt(y / normalizer)\n",
    "    xye = np.column_stack((x, y, esd))\n",
    "    name, suffix = filename.split(\".\")\n",
    "    output_name = f\"{name}_esd.{suffix}\"\n",
    "    np.savetxt(output_path / output_name,\n",
    "               xye,\n",
    "               fmt=\"%.18e\",\n",
    "               delimiter=\"\\t\",\n",
    "               encoding=\"utf-8\",\n",
    "               )\n",
    "\n",
    "    return xye, output_path / output_name"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading wavelength (wl) and sample-detector-distance (sdd) (in meters) from \n",
    "`.poni` file from calibration.  \n",
    "Converting wl to √Öngstr√∂m (√Ö) and sdd to millimeters (mm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai = pyFAI.load(str(poni_file))\n",
    "wl, sdd = ai.wavelength * 10**10, ai.dist * 10**3\n",
    "print(f\"{80*'-'}\\nwl: {wl} √Ö\\nsdd: {sdd} mm\\n{80*'-'}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimating esd for calibrant and writing to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_calib_file = list(output_calib_path.glob(f\"*.{output_format}\"))[0]\n",
    "print(f\"{80*'-'}\\nWriting esds for calibrant...\")\n",
    "d_calib[\"xye\"], d_calib[\"esd_path\"] = esd_calc_write(output_calib_file.name,\n",
    "                                                     d_calib[\"x\"],\n",
    "                                                     d_calib[\"y\"],\n",
    "                                                     wl,\n",
    "                                                     sdd,\n",
    "                                                     calib_esd_path,\n",
    "                                                     )\n",
    "print(f\"Done. Please see the '{calib_esd_path.name}' folder.\\n{80*'-'}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimating esd for background and writing to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_bkg_file = list(output_bkg_path.glob(f\"*.{output_format}\"))[0]\n",
    "print(f\"{80*'-'}\\nWriting esds for bkg...\")\n",
    "d_bkg[\"xye\"], d_bkg[\"esd_path\"] = esd_calc_write(output_bkg_file.name,\n",
    "                                                 d_bkg[\"x\"],\n",
    "                                                 d_bkg[\"y\"],\n",
    "                                                 wl,\n",
    "                                                 sdd,\n",
    "                                                 bkg_esd_path,\n",
    "                                                 )\n",
    "print(f\"Done. Please see the '{bkg_esd_path.name}' folder.\\n{80*'-'}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimating esd for data files and writing to files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{80*'-'}\\nCalculating esds and writing to files...\")\n",
    "for i, k in enumerate(d_data.keys()):\n",
    "    input_file, x, y = d_data[k][\"input_file\"], d_data[k][\"x\"], d_data[k][\"y\"]\n",
    "    input_file = output_path / f\"{input_file.stem}.{output_format}\"\n",
    "    print(f\"\\t{i}\\t{input_file.stem}_esd{input_file.suffix}\")\n",
    "    d_data[k][\"xye\"], d_data[k][\"esd_path\"] = esd_calc_write(input_file.name, \n",
    "                                                             x, \n",
    "                                                             y, \n",
    "                                                             wl, \n",
    "                                                             sdd, \n",
    "                                                             data_esd_path,\n",
    "                                                             )\n",
    "print(f\"Done. Please see the '{data_esd_path.name}' folder.\\n{80*'-'}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating output folders for normalized background and data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_esd_norm_path = Path.cwd() / f\"{output_format}_esd_norm\"\n",
    "bkg_esd_norm_path = Path.cwd() / f\"{output_format}_bkg_esd_norm\"\n",
    "for p in [data_esd_norm_path, bkg_esd_norm_path]:\n",
    "    if not p.exists():\n",
    "        p.mkdir()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function definitions used for normalization/scaling of background and data \n",
    "files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basecase_calc(x, y, xmin, xmax):\n",
    "    basecase = 0\n",
    "    for i in range(len(x)):\n",
    "        if xmin <= x[i] <= xmax:\n",
    "            basecase += y[i]\n",
    "\n",
    "    return basecase\n",
    "\n",
    "\n",
    "def norm_factor(x, y, basecase, xmin, xmax):\n",
    "    intensity, intensity_sum = 0, 0\n",
    "    for i in range(len(y)):\n",
    "        if xmin <= x[i] <= xmax:\n",
    "            intensity_sum += y[i]\n",
    "    if intensity_sum == 0:\n",
    "        normalizer = 1\n",
    "    else:\n",
    "        normalizer = basecase / intensity_sum\n",
    "\n",
    "    return normalizer\n",
    "\n",
    "\n",
    "def normalizer(intput_file, xye, basecase, xmin, xmax, output_path):\n",
    "    x, y, e = xye.T\n",
    "    factor = norm_factor(x, y, basecase, xmin, xmax)\n",
    "    y_norm, e_norm = y * factor, e * factor\n",
    "    xye_norm = np.column_stack((x, y_norm, e_norm))\n",
    "    output_name = f\"{input_file.stem}_norm{input_file.suffix}\"\n",
    "    np.savetxt(output_path / output_name,\n",
    "               xye_norm,\n",
    "               fmt=\"%.18e\",\n",
    "               delimiter=\"\\t\",\n",
    "               encoding=\"utf-8\",\n",
    "               )\n",
    "\n",
    "    return xye_norm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization/scaling of data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{80*'-'}\\nNormalizing data files...\")\n",
    "for i, k in enumerate(d_data.keys()):\n",
    "    input_file, xye = d_data[k][\"esd_path\"], d_data[k][\"xye\"]\n",
    "    print(f\"\\t{i}\\t{d_data[k]['esd_path'].name}\")\n",
    "    x, y, e = xye.T\n",
    "    if i == 0:\n",
    "        basecase = basecase_calc(x, y, xmin, xmax)\n",
    "    d_data[k][\"xye_norm\"] = normalizer(input_file, \n",
    "                                       xye, \n",
    "                                       basecase, \n",
    "                                       xmin, \n",
    "                                       xmax, \n",
    "                                       data_esd_norm_path,\n",
    "                                       )\n",
    "print(f\"Done. Please see the '{data_esd_norm_path.name}' folder.\\n{80*'-'}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization/scaling of background file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{80*'-'}\\nNormalizing background file...\\n\\t{d_bkg['esd_path'].name}\")\n",
    "input_file, xye = d_bkg[\"esd_path\"], d_bkg[\"xye\"]\n",
    "x, y, e = xye.T\n",
    "d_bkg[\"xye_norm\"] = normalizer(input_file, \n",
    "                               xye, \n",
    "                               basecase, \n",
    "                               xmin, \n",
    "                               xmax, \n",
    "                               bkg_esd_norm_path,\n",
    "                               )\n",
    "print(f\"Done. Please see the '{bkg_esd_norm_path.name}' folder.\\n{80*'-'}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating output folders for plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_folders = [\"png\", \"pdf\", \"svg\"]\n",
    "plot_paths = [Path.cwd() / folder for folder in plot_folders]\n",
    "for p in plot_paths:\n",
    "    if not p.exists():\n",
    "        p.mkdir()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting normalized/scaled background and data files together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "print(f\"{80*'-'}\\nPlotting normalized/scaled background and data files \"\n",
    "      f\"together.\")\n",
    "keys = list(d_data.keys())\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "x_bkg, y_bkg, e_bkg = d_bkg[\"xye_norm\"].T\n",
    "ax.plot(x_bkg, y_bkg, c=\"k\", lw=1, label=\"bkg\")\n",
    "for i, k in enumerate(keys):\n",
    "    x, y, e = d_data[k][\"xye_norm\"].T\n",
    "    ax.plot(x, y, c=np.random.rand(3,), lw=1)\n",
    "ax.set_xlim(np.amin(x_bkg), np.amax(x_bkg))\n",
    "ax.set_xlabel(\"$Q\\;[\\mathrm{\\AA}^{-1}]$\", fontsize=14)\n",
    "ax.set_ylabel(\"$I\\;[\\mathrm{arb.\\;u.}]$\", fontsize=14)\n",
    "ax.tick_params(axis=\"both\", labelsize=12)\n",
    "ax.legend()\n",
    "for p in plot_paths:\n",
    "    print(f\"\\t{p.name}\")\n",
    "    plt.savefig(p / f\"data_bkg_norm.{p.name}\", bbox_inches=\"tight\", dpi=600)\n",
    "print(f\"Done. Please see the {plot_folders} folders.\\n{80*'-'}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overview plot for full Q-range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "print(f\"{80*'-'}\\nOverview plot for full Q-range...\")\n",
    "for i, k in enumerate(d_data.keys()):\n",
    "    x, y, e = d_data[k][\"xye_norm\"].T\n",
    "    if i == 0:\n",
    "        x_array = x\n",
    "        y_array = y        \n",
    "    else:\n",
    "        y_array = np.column_stack((y_array, y))\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "im = ax.imshow(y_array,\n",
    "               aspect=\"auto\", \n",
    "               extent=(0, \n",
    "                       len(d_data.keys()), \n",
    "                       np.amax(x_array), \n",
    "                       np.amin(x_array)),\n",
    "                       )\n",
    "ax.tick_params(axis=\"x\",\n",
    "               which=\"both\",\n",
    "               top=True,\n",
    "               labeltop=True,\n",
    "               bottom=False,\n",
    "               labelbottom=False,\n",
    "               )\n",
    "ax.tick_params(axis=\"both\", labelsize=14)\n",
    "ax.minorticks_on()\n",
    "ax.set_xlabel(\"scan number\", fontsize=20)\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "ax.set_ylabel(\"$Q\\;[\\mathrm{\\AA}^{-1}]$\", fontsize=20)\n",
    "cbar = plt.colorbar(im)\n",
    "cbar.set_label(\"$I\\;[\\mathrm{arb.\\;u.}]$\", fontsize=20)\n",
    "cbar.ax.tick_params(labelsize=14)\n",
    "cbar.formatter.set_powerlimits((0, 0))\n",
    "for p in plot_paths:\n",
    "    print(f\"\\t{p.name}\")\n",
    "    plt.savefig(p / f\"data_overview_full.{p.name}\", \n",
    "                bbox_inches=\"tight\", \n",
    "                dpi=600,\n",
    "                )\n",
    "print(f\"Done. Please see the {plot_folders} folders.\\n{80*'-'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmin_plot, xmax_plot = 1.0, 5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "print(f\"{80*'-'}\\nPlotting overview plot from {xmin_plot} to {xmax_plot} \"\n",
    "      f\"√Ö^-1...\")\n",
    "xmin_idx, xmax_idx = 0, -1\n",
    "keys = list(d_data.keys())\n",
    "for i, x in enumerate(d_data[keys[0]][\"x\"]):\n",
    "    if xmin_plot <= x:\n",
    "        xmin_idx = i\n",
    "        break\n",
    "for i, x in enumerate(d_data[keys[0]][\"x\"]):\n",
    "    if xmax_plot <= x:\n",
    "        xmax_idx = i\n",
    "        break\n",
    "for i, k in enumerate(d_data.keys()):\n",
    "    x, y, e = d_data[k][\"xye_norm\"].T\n",
    "    if i == 0:\n",
    "        x_array = x[xmin_idx:xmax_idx]\n",
    "        y_array = y[xmin_idx:xmax_idx]\n",
    "    else:\n",
    "        y = np.column_stack((y_array, y[xmin_idx:xmax_idx]))\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "im = ax.imshow(y,\n",
    "               aspect=\"auto\", \n",
    "               extent=(0, \n",
    "                       len(d_data.keys()), \n",
    "                       np.amax(x_array), \n",
    "                       np.amin(x_array)\n",
    "                       ),\n",
    "               )\n",
    "ax.tick_params(axis=\"x\",\n",
    "               which=\"both\",\n",
    "               top=True,\n",
    "               labeltop=True,\n",
    "               bottom=False,\n",
    "               labelbottom=False,\n",
    "               )\n",
    "ax.tick_params(axis=\"both\", labelsize=14)\n",
    "ax.minorticks_on()\n",
    "ax.set_xlabel(\"scan number\", fontsize=20)\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "ax.set_ylabel(\"$Q\\;[\\mathrm{\\AA}^{-1}]$\", fontsize=20)\n",
    "cbar = plt.colorbar(im)\n",
    "cbar.set_label(\"$I\\;[\\mathrm{arb.\\;u.}]$\", fontsize=20)\n",
    "cbar.ax.tick_params(labelsize=14)\n",
    "cbar.formatter.set_powerlimits((0, 0))\n",
    "for p in plot_paths:\n",
    "    print(f\"\\t{p.name}\")\n",
    "    plt.savefig(p / f\"data_overview_{xmin_plot}-{xmax_plot}.{p.name}\", \n",
    "                bbox_inches=\"tight\", \n",
    "                dpi=600,\n",
    "                )\n",
    "print(f\"Done. Please see the {plot_folders} folders.\\n{80*'-'}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function definition to obtain scale factor for background to avoid negative \n",
    "intensities for background-subtracted data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bkg_scale_factor(d_data, d_bkg):\n",
    "    bkg_scale_list = []\n",
    "    bkg_scale = 1\n",
    "    x_bkg, y_bkg, e_bkg = d_bkg[\"xye_norm\"].T\n",
    "    for k in d_data.keys():\n",
    "        x, y, e = d_data[k][\"xye_norm\"].T\n",
    "        y_bkg_scaled = bkg_scale * y_bkg\n",
    "        # bool_list = y >= y_bkg\n",
    "        y_bool = np.any((y - y_bkg_scaled < 0))\n",
    "        while y_bool:\n",
    "            bkg_scale = bkg_scale * 0.999\n",
    "            y_bkg_scaled = bkg_scale * y_bkg\n",
    "            # bool_list = y >= y_bkg\n",
    "            y_bool = np.any((y - y_bkg_scaled < 0))\n",
    "        bkg_scale_list.append(bkg_scale)\n",
    "\n",
    "    return min(bkg_scale_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtaining background scale factor and writing it to `.txt` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{80*'-'}\\nObtaining scale factor for background...\")\n",
    "bkg_scale = bkg_scale_factor(d_data, d_bkg)\n",
    "bkg_scale_output = Path.cwd() / \"bkg_scale.txt\"\n",
    "with bkg_scale_output.open(mode=\"w\") as o:\n",
    "    o.write(f\"bkgscale\\t{bkg_scale}\\n\")\n",
    "print(f\"\\tbkg_scale: {bkg_scale}\\nWritten to {bkg_scale_output.name}.\"\n",
    "      f\"\\n{80*'-'}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating output path for background-subtracted data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_esd_norm_bkgsub_path = Path.cwd() / f\"{output_format}_esd_norm_bkgsub\"\n",
    "if not data_esd_norm_bkgsub_path.exists():\n",
    "    data_esd_norm_bkgsub_path.mkdir()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subtracting scaled background from data and writing to files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{80*'-'}\\nSubtracting background from data...\")\n",
    "x_bkg, y_bkg, e_bkg = d_bkg[\"xye_norm\"].T\n",
    "y_bkg_scaled = bkg_scale * y_bkg\n",
    "for i, k in enumerate(d_data.keys()):\n",
    "    print(f\"\\t{i}\\t{d_data[k]['input_file'].stem}.{output_format}\")\n",
    "    x, y, e = d_data[k][\"xye_norm\"].T\n",
    "    y_bkgsub = y - y_bkg_scaled\n",
    "    d_data[k][\"y_bkgsub\"] = y_bkgsub\n",
    "    outputname = f\"{d_data[k]['input_file'].stem}.{output_format}\" \n",
    "    np.savetxt(data_esd_norm_bkgsub_path / outputname,\n",
    "               np.column_stack((x, y_bkgsub, e)),\n",
    "               fmt=\"%.18e\",\n",
    "               delimiter=\"\\t\",\n",
    "               encoding=\"utf-8\",\n",
    "               )\n",
    "print(f\"Done. Please see the '{data_esd_norm_bkgsub_path.name}' folder.\"\n",
    "      f\"\\n{80*'-'}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting background-subtracted data files together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "print(f\"{80*'-'}\\nPlotting background-subtracted data files together...\")\n",
    "keys = list(d_data.keys())\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "for i, k in enumerate(keys):\n",
    "    ax.plot(d_data[k][\"x\"], d_data[k][\"y_bkgsub\"], c=np.random.rand(3,), lw=1)\n",
    "ax.set_xlim(np.amin(d_data[k][\"x\"]), np.amax(d_data[k][\"x\"]))\n",
    "ax.set_xlabel(\"$Q\\;[\\mathrm{\\AA}^{-1}]$\", fontsize=14)\n",
    "ax.set_ylabel(\"$I\\;[\\mathrm{arb.\\;u.}]$\", fontsize=14)\n",
    "ax.tick_params(axis=\"both\", labelsize=12)\n",
    "for p in plot_paths:\n",
    "    print(f\"\\t{p.name}\")\n",
    "    plt.savefig(p / f\"data_bkgsub.{p.name}\", bbox_inches=\"tight\", dpi=600)\n",
    "print(f\"Done. Please see the {plot_folders} folder.\\n{80*'-'}\")  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overview plot for background-subtracted data and full Q-range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "print(f\"{80*'-'}\\nOverview plot for background-subtracted data and full \"\n",
    "      f\"Q-range...\")\n",
    "for i, k in enumerate(d_data.keys()):\n",
    "    if i == 0:\n",
    "        y = d_data[k][\"y_bkgsub\"]\n",
    "        x = d_data[k][\"x\"]\n",
    "    else:\n",
    "        y = np.column_stack((y, d_data[k][\"y_bkgsub\"]))\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "im = ax.imshow(y, \n",
    "               aspect=\"auto\", \n",
    "               extent=(0, len(d_data.keys()), np.amax(x), np.amin(x)),\n",
    "               )\n",
    "ax.tick_params(axis=\"x\",\n",
    "               which=\"both\",\n",
    "               top=True,\n",
    "               labeltop=True,\n",
    "               bottom=False,\n",
    "               labelbottom=False,\n",
    "               )\n",
    "ax.tick_params(axis=\"both\", labelsize=14)\n",
    "ax.minorticks_on()\n",
    "ax.set_xlabel(\"scan number\", fontsize=20)\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "ax.set_ylabel(\"$Q\\;[\\mathrm{\\AA}^{-1}]$\", fontsize=20)\n",
    "cbar = plt.colorbar(im)\n",
    "cbar.set_label(\"$I\\;[\\mathrm{arb.\\;u.}]$\", fontsize=20)\n",
    "cbar.ax.tick_params(labelsize=14)\n",
    "cbar.formatter.set_powerlimits((0, 0))\n",
    "for p in plot_paths:\n",
    "    print(f\"\\t{p.name}\")\n",
    "    plt.savefig(p / f\"data_bkgsub_overview_full.{p.name}\", \n",
    "                bbox_inches=\"tight\", \n",
    "                dpi=600,\n",
    "                )\n",
    "print(f\"Done. Please see the {plot_folders} folders.\\n{80*'-'}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve contrast in the overview plots, please specify minimum and maximum\n",
    "Q-values to include in the overview plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmin_plot, xmax_plot = 1.0, 5.0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overview plot for background-subtracted data and specified Q-range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "print(f\"{80*'-'}\\nPlotting overview plot from {xmin_plot} to {xmax_plot} \"\n",
    "      f\"√Ö^-1...\")\n",
    "xmin_idx, xmax_idx = 0, -1\n",
    "keys = list(d_data.keys())\n",
    "for i, x in enumerate(d_data[keys[0]][\"x\"]):\n",
    "    if xmin_plot <= x:\n",
    "        xmin_idx = i\n",
    "        break\n",
    "for i, x in enumerate(d_data[keys[0]][\"x\"]):\n",
    "    if xmax_plot <= x:\n",
    "        xmax_idx = i\n",
    "        break\n",
    "for i, k in enumerate(d_data.keys()):\n",
    "    if i == 0:\n",
    "        y = d_data[k][\"y_bkgsub\"][xmin_idx:xmax_idx]\n",
    "        x = d_data[k][\"x\"][xmin_idx:xmax_idx]\n",
    "    else:\n",
    "        y = np.column_stack((y, d_data[k][\"y_bkgsub\"][xmin_idx:xmax_idx]))\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "im = ax.imshow(y, \n",
    "               aspect=\"auto\", \n",
    "               extent=(0, len(d_data.keys()), np.amax(x), np.amin(x)),\n",
    "               )\n",
    "ax.tick_params(axis=\"x\",\n",
    "               which=\"both\",\n",
    "               top=True,\n",
    "               labeltop=True,\n",
    "               bottom=False,\n",
    "               labelbottom=False,\n",
    "               )\n",
    "ax.tick_params(axis=\"both\", labelsize=14)\n",
    "ax.minorticks_on()\n",
    "ax.set_xlabel(\"scan number\", fontsize=20)\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "ax.set_ylabel(\"$Q\\;[\\mathrm{\\AA}^{-1}]$\", fontsize=20)\n",
    "cbar = plt.colorbar(im)\n",
    "cbar.set_label(\"$I\\;[\\mathrm{arb.\\;u.}]$\", fontsize=20)\n",
    "cbar.ax.tick_params(labelsize=14)\n",
    "cbar.formatter.set_powerlimits((0, 0))\n",
    "for p in plot_paths:\n",
    "    print(f\"\\t{p.name}\")\n",
    "    plt.savefig(p / f\"data_bkgsub_overview_{xmin_plot}-{xmax_plot}.{p.name}\", \n",
    "                bbox_inches=\"tight\", \n",
    "                dpi=600,\n",
    "                )\n",
    "print(f\"Done. Please see the {plot_folders} folders.\\n{80*'-'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This concludes this pipeline. Good luck with your data analysis!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masking_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
